{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import operator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "os.chdir(\"/Users/Yanean/Desktop/ML/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(filepath):\n",
    "    feature = np.array(pd.read_csv(filepath, header = None))\n",
    "    feature_dict = {}\n",
    "    for i in range(len(feature)):\n",
    "        id = int(feature[i][0].split(\"/\")[1].split(\".\")[0])\n",
    "        feature_dict[id] = np.array(feature[i][1:], dtype=float)\n",
    "    res = []\n",
    "    for i in range(len(feature_dict)):\n",
    "        res.append(feature_dict[i])\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_featfc = load_features(\"./features_train/features_resnet1000_train.csv\")\n",
    "test_featfc = load_features(\"./features_test/features_resnet1000_test.csv\")\n",
    "train_featpool=load_features(\"./features_train/features_resnet1000intermediate_train.csv\")\n",
    "test_featpool=load_features(\"./features_test/features_resnet1000intermediate_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "total_tag_dict = list()\n",
    "def get_tag_dict(path):\n",
    "    # open all description trianing file(5 sentences txt)\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            sentence = content.split(\"\\n\")\n",
    "            for k in sentence:\n",
    "                sentence2 = k.split(\":\")\n",
    "                for str in sentence2:\n",
    "                    if str not in total_tag_dict and str != \"\":\n",
    "                        total_tag_dict.append(str)\n",
    "get_tag_dict(\"./tags_train/\")\n",
    "print (len(total_tag_dict))\n",
    "print (total_tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {} # HashMap (word, id), word stands for high frequency word while id is assigned to it\n",
    "index = 0\n",
    "for w in total_tag_dict:\n",
    "    tag_dict[w] = index\n",
    "    index += 1 \n",
    "print (len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "total_word_dict = {}\n",
    "def get_word_dict(path):\n",
    "    # open all description trianing file(5 sentences txt)\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            # Lemmatizing & Remove Stop Words\n",
    "            sentence = filter(None, re.split(r'[^a-zA-Z\\']', content.lower()))\n",
    "            stop = set(stopwords.words('english'))\n",
    "            #print stop\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            parsed = [lmtzr.lemmatize(word) for word in sentence if word not in stop]\n",
    "            #print parsed\n",
    "            for word in parsed:\n",
    "                #w = stemmer.stem(word)\n",
    "                if word in total_word_dict:\n",
    "                    total_word_dict[word] += 1\n",
    "                else:\n",
    "                    total_word_dict[word] = 1\n",
    "# get the word dictionary for training description\n",
    "# HashMap: (word, frequency)\n",
    "get_word_dict(\"./descriptions_train/\")\n",
    "\n",
    "print (total_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {} # HashMap (word, id), word stands for high frequency word while id is assigned to it\n",
    "index = 0\n",
    "# for all words in dictionary, if frequency > threshold, identify it as a high frequency word \n",
    "# assign an index to it, and increase the index\n",
    "for w in total_word_dict:\n",
    "    if total_word_dict[w] > 10:\n",
    "        word_dict[w] = index\n",
    "        index += 1 \n",
    "print (word_dict)\n",
    "print (len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dict = {}\n",
    "index = 0\n",
    "for w in word_dict:\n",
    "    if w not in total_dict:\n",
    "        total_dict[w] = index\n",
    "        index = index + 1\n",
    "for w in tag_dict:\n",
    "    if w not in total_dict:\n",
    "        total_dict[w] = index\n",
    "        index = index + 1\n",
    "        \n",
    "# print len(total_dict)\n",
    "# print (total_dict)\n",
    "\n",
    "def process_des(path, thresh):\n",
    "    #HashMap (filename, array)\n",
    "    # filename example could be 1.txt etc. \n",
    "    # array should be a 1644 length, telling us for this specific file (ex. 1.txt)\n",
    "    # the frequency of all high frequency word (for example, the word yellow appears 12 times in total)\n",
    "    # but in the current file, it appears 3 times\n",
    "    des_vec = {}\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            # Lemmatizing & Remove Stop Words\n",
    "            cur = [0] * len(total_dict)\n",
    "            sentence = filter(None, re.split(r'[^a-zA-Z\\']', content.lower()))\n",
    "            stop = set(stopwords.words('english'))\n",
    "            #print stop\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            parsed = [lmtzr.lemmatize(word) for word in sentence if word not in stop]\n",
    "            #print parsed\n",
    "            for word in parsed:\n",
    "                #w = stemmer.stem(word)\n",
    "                if word in total_dict:\n",
    "                    cur[total_dict[word]] = 1\n",
    "            des_vec[filename.split('/')[-1]] = cur\n",
    "    return des_vec # ex. {\"0.txt\": [1,0,0,2,0,0,0,0,.....]; \"1.txt\" : [2,1,1,1,0,0,1,....]}\n",
    "\n",
    "thresh = 10\n",
    "train_des = process_des(\"./descriptions_train/\", thresh)\n",
    "print (train_des[\"0.txt\"])\n",
    "print (len(train_des[\"0.txt\"]))\n",
    "test_des = process_des(\"./descriptions_test/\", thresh)\n",
    "#print test_des[\"0.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_des(path):\n",
    "    des_vec = {}\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            sentence = content.split(\"\\n\")\n",
    "            cur = [0] * len(total_tag_dict)\n",
    "            for k in sentence:\n",
    "                sentence2 = k.split(\":\")\n",
    "                for str in sentence2:\n",
    "                    #print str\n",
    "                    if str in total_tag_dict:\n",
    "                        cur[tag_dict[str]] = 1\n",
    "            des_vec[filename.split('/')[-1]] = cur\n",
    "    return des_vec \n",
    "\n",
    "train_tag_des = process_des(\"./tags_train/\")\n",
    "print (type(train_tag_des[\"0.txt\"]))\n",
    "test_tag_des = process_des(\"./tags_test/\")\n",
    "#print test_des[\"0.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = [ ]\n",
    "for k in train_tag_des:\n",
    "    Y.append(train_tag_des[k][:-3])\n",
    "# print(Y)\n",
    "len(Y)\n",
    "Y = np.array(Y, dtype = np.float32)\n",
    "#      np.concatenate((Y, train_tag_des[k]), axis=0)\n",
    "    #Y.append(train_tag_des[k])\n",
    "#     filename = int(k.split(\".\")[0])\n",
    "    #print len(train_featfc_map[filename].reshape(-1, 1))\n",
    "#     Y = np.concatenate((Y, train_featfc_map[filename]), axis=0)\n",
    "#     Y = np.concatenate((Y, train_featpool_map[filename]), axis=0)\n",
    "    #Y.append(train_featpool_map[filename])\n",
    "#     print k\n",
    "#     print i\n",
    "#     i = i +1\n",
    "# Y = np.array(Y, dtype = np.float32)\n",
    "\n",
    "Y= np.concatenate((Y, train_featfc), axis=1)\n",
    "Y = np.concatenate((Y, train_featpool), axis=1)\n",
    "Y = np.array(Y, dtype = np.float32)\n",
    "# print (Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first part for SVM\n",
    "X = []\n",
    "for k in train_des:\n",
    "    #train_des[k].append(k)\n",
    "    X.append(train_des[k])\n",
    "X = np.array(X, dtype = np.float32)\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76393553\n",
      "Iteration 2, loss = 0.71543404\n",
      "Iteration 3, loss = 0.71175271\n",
      "Iteration 4, loss = 0.70606606\n",
      "Iteration 5, loss = 0.69744292\n",
      "Iteration 6, loss = 0.68407222\n",
      "Iteration 7, loss = 0.66627243\n",
      "Iteration 8, loss = 0.65002539\n",
      "Iteration 9, loss = 0.63140882\n",
      "Iteration 10, loss = 0.59320141\n",
      "Iteration 11, loss = 0.55206068\n",
      "Iteration 12, loss = 0.51918393\n",
      "Iteration 13, loss = 0.48775329\n",
      "Iteration 14, loss = 0.45677375\n",
      "Iteration 15, loss = 0.43511087\n",
      "Iteration 16, loss = 0.41895702\n",
      "Iteration 17, loss = 0.40445130\n",
      "Iteration 18, loss = 0.39101383\n",
      "Iteration 19, loss = 0.37955579\n",
      "Iteration 20, loss = 0.36868184\n",
      "[[ 0.22834304  0.04068868  0.14662281 ...,  0.27115732  0.56784736\n",
      "   0.36727272]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:565: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "# train_x = total_bow[0:10000] # 这里是training的description\n",
    "# train_y = vis # 这里是train的resNet\n",
    "# 在这里加入模型\n",
    "reg = MLPRegressor(hidden_layer_sizes=(1000,1000,1000),max_iter = 500,alpha=0.0001,\n",
    "                   solver='sgd',verbose=10,random_state=21,tol = 0.0000001)\n",
    "# reg = MLPRegressor(solver='sgd', activation='logistic', shuffle=True)\n",
    "reg.fit(X, Y)\n",
    "p = reg.predict(test_des[\"0.txt\"])\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1822)\n",
      "(10000, 88)\n",
      "(2000, 1000)\n",
      "(2000, 2048)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-81a8179d28c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_featfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_featpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_featfc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_featpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "#print p.shape\n",
    "X_test = []\n",
    "for k in test_des:\n",
    "    #train_des[k].append(k)\n",
    "    X_test.append(test_des[k])\n",
    "X_test = np.array(X_test, dtype = np.float32)\n",
    "print (X_test.shape)\n",
    "\n",
    "Y_test = []\n",
    "for k in test_tag_des:\n",
    "    #train_tag_des[k].append(k)\n",
    "    #print k\n",
    "    # k的value很重要 你看到k就知道这是哪一个txt，然后你根据k的value去hashmap里面找相关test_featpc和test_featpool的vector\n",
    "    # 这个hashmap 你在一开始读入文件的时候就可以build一下  我把第0行给删了 其实第0行还是有些用处的 可以用来build hashmap\n",
    "    Y_test.append(test_tag_des[k][:-3])\n",
    "Y_test = np.array(Y_test, dtype = np.float32)\n",
    "print (Y_test.shape)\n",
    "print(test_featfc.shape)\n",
    "print(test_featpool.shape)\n",
    "Y_test = np.concatenate((Y_test, test_featfc), axis=1)\n",
    "Y_test = np.concatenate((Y_test, test_featpool), axis=1)\n",
    "print (Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for k in range(2000):\n",
    "    fileName = str(k)+'.txt'\n",
    "    p = reg.predict(np.array(test_des[fileName]).reshape(1,-1))\n",
    "    pred.append(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for k in range(2000):\n",
    "    fileName = str(k)+'.txt'\n",
    "    p = reg.predict(np.array(test_des[fileName]).reshape(1,-1))\n",
    "    pred.append(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=20, algorithm='ball_tree').fit(Y_test)\n",
    "print(pred)\n",
    "result=nbrs.kneighbors(np.array(pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time,csv\n",
    "def get_timestamp():\n",
    "    millis = int(round(time.time() * 1000))\n",
    "    return millis\n",
    "\n",
    "def create_submission_csv(predictions):\n",
    "    filename = 'submission' + '_' + str(get_timestamp()) + '.csv'\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(['Descritpion_ID', 'Top_20_Image_IDs'])\n",
    "        for i in range (0, len(predictions)):\n",
    "            description_id = str(i) + \".txt\"\n",
    "            top20_image_ids = \" \".join(str(index) + '.jpg' for index in predictions[i])\n",
    "            writer.writerow([description_id, top20_image_ids])\n",
    "create_submission_csv(result[1])            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
