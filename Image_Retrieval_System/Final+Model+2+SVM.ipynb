{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1001)\n",
      "(10000, 1001)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import operator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "os.chdir(\"/Users/Ruihao/Desktop/course/AML/Final/data\")\n",
    "\n",
    "train_feat, test_feat = [], []\n",
    "for line in open(\"./features_train/features_resnet1000_train.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    train_feat.append(l)\n",
    "train_feat = np.array(train_feat)\n",
    "for line in open(\"./features_test/features_resnet1000_test.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    test_feat.append(l)\n",
    "test_feat = np.array(test_feat)\n",
    "\n",
    "print test_feat.shape\n",
    "print train_feat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "['indoor', 'teddy bear', '', 'animal', 'cat', 'electronic', 'cell phone', 'person', 'accessory', 'handbag', 'furniture', 'dining table', 'kitchen', 'knife', 'food', 'carrot', 'book', 'wine glass', 'fork', 'cake', 'chair', 'pizza', 'umbrella', 'outdoor', 'stop sign', 'sports', 'kite', 'giraffe', 'potted plant', 'appliance', 'refrigerator', 'bowl', 'apple', 'orange', 'microwave', 'oven', 'toaster', 'spoon', 'sink', 'vase', 'vehicle', 'train', 'motorcycle', 'bottle', 'bed', 'boat', 'tv', 'remote', 'zebra', 'tennis racket', 'skis', 'suitcase', 'backpack', 'snowboard', 'dog', 'banana', 'keyboard', 'mouse', 'sheep', 'broccoli', 'scissors', 'cup', 'tie', 'airplane', 'bear', 'laptop', 'clock', 'elephant', 'car', 'skateboard', 'bird', 'bench', 'surfboard', 'horse', 'truck', 'bus', 'traffic light', 'baseball bat', 'baseball glove', 'couch', 'cow', 'bicycle', 'sports ball', 'fire hydrant', 'sandwich', 'hot dog', 'toothbrush', 'donut', 'toilet', 'parking meter', 'frisbee', 'hair drier']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "total_tag_dict = list()\n",
    "def get_tag_dict(path):\n",
    "    # open all description trianing file(5 sentences txt)\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            sentence = content.split(\"\\n\")\n",
    "            for k in sentence:\n",
    "                sentence2 = k.split(\":\")\n",
    "                for str in sentence2:\n",
    "                    if str not in total_tag_dict:\n",
    "                        total_tag_dict.append(str)\n",
    "get_tag_dict(\"./tags_train/\")\n",
    "print len(total_tag_dict)\n",
    "print total_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {} # HashMap (word, id), word stands for high frequency word while id is assigned to it\n",
    "index = 0\n",
    "for w in total_tag_dict:\n",
    "    tag_dict[w] = index\n",
    "    index += 1 \n",
    "print len(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "total_word_dict = {}\n",
    "def get_word_dict(path):\n",
    "    # open all description trianing file(5 sentences txt)\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            # Lemmatizing & Remove Stop Words\n",
    "            sentence = filter(None, re.split(r'[^a-zA-Z\\']', content.lower()))\n",
    "            stop = set(stopwords.words('english'))\n",
    "            #print stop\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            parsed = [lmtzr.lemmatize(word) for word in sentence if word not in stop]\n",
    "            #print parsed\n",
    "            for word in parsed:\n",
    "                #w = stemmer.stem(word)\n",
    "                if word in total_word_dict:\n",
    "                    total_word_dict[word] += 1\n",
    "                else:\n",
    "                    total_word_dict[word] = 1\n",
    "# get the word dictionary for training description\n",
    "# HashMap: (word, frequency)\n",
    "get_word_dict(\"./descriptions_train/\")\n",
    "#print total_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804\n"
     ]
    }
   ],
   "source": [
    "word_dict = {} # HashMap (word, id), word stands for high frequency word while id is assigned to it\n",
    "index = 0\n",
    "# for all words in dictionary, if frequency > threshold, identify it as a high frequency word \n",
    "# assign an index to it, and increase the index\n",
    "for w in total_word_dict:\n",
    "    if total_word_dict[w] > 10:\n",
    "        word_dict[w] = index\n",
    "        index += 1 \n",
    "print len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "def process_des(path, thresh):\n",
    "    #HashMap (filename, array)\n",
    "    # filename example could be 1.txt etc. \n",
    "    # array should be a 1644 length, telling us for this specific file (ex. 1.txt)\n",
    "    # the frequency of all high frequency word (for example, the word yellow appears 12 times in total)\n",
    "    # but in the current file, it appears 3 times\n",
    "    des_vec = {}\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            # Lemmatizing & Remove Stop Words\n",
    "            cur = [0] * len(total_tag_dict)\n",
    "            sentence = filter(None, re.split(r'[^a-zA-Z\\']', content.lower()))\n",
    "            stop = set(stopwords.words('english'))\n",
    "            #print stop\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            parsed = [lmtzr.lemmatize(word) for word in sentence if word not in stop]\n",
    "            #print parsed\n",
    "            for word in parsed:\n",
    "                #w = stemmer.stem(word)\n",
    "                if word in total_tag_dict:\n",
    "                    cur[tag_dict[word]] = 1\n",
    "            des_vec[filename.split('/')[-1]] = cur\n",
    "    return des_vec # ex. {\"0.txt\": [1,0,0,2,0,0,0,0,.....]; \"1.txt\" : [2,1,1,1,0,0,1,....]}\n",
    "\n",
    "thresh = 10\n",
    "train_des = process_des(\"./descriptions_train/\", thresh)\n",
    "print train_des[\"0.txt\"]\n",
    "print len(train_des[\"0.txt\"])\n",
    "test_des = process_des(\"./descriptions_test/\", thresh)\n",
    "#print test_des[\"0.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 92)\n"
     ]
    }
   ],
   "source": [
    "# first part for SVM\n",
    "X = []\n",
    "for k in train_des:\n",
    "    #train_des[k].append(k)\n",
    "    X.append(train_des[k])\n",
    "X = np.array(X)\n",
    "\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def process_des(path):\n",
    "    des_vec = {}\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            sentence = content.split(\"\\n\")\n",
    "            cur = [0] * len(total_tag_dict)\n",
    "            for k in sentence:\n",
    "                sentence2 = k.split(\":\")\n",
    "                for str in sentence2:\n",
    "                    #print str\n",
    "                    if str in total_tag_dict:\n",
    "                        cur[tag_dict[str]] = 1\n",
    "            des_vec[filename.split('/')[-1]] = cur\n",
    "    return des_vec \n",
    "\n",
    "train_tag_des = process_des(\"./tags_train/\")\n",
    "print train_tag_des[\"0.txt\"]\n",
    "test_tag_des = process_des(\"./tags_test/\")\n",
    "#print test_des[\"0.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# second part for SVM\n",
    "Y = []\n",
    "for k in train_tag_des:\n",
    "    #train_tag_des[k].append(k)\n",
    "    Y.append(train_tag_des[k])\n",
    "Y = np.array(Y)\n",
    "print type(Y)\n",
    "# print len(Y)\n",
    "# print len(Y[0])\n",
    "#print Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 92)\n",
      "(10000, 92)\n",
      "(4, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "print Y.shape\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "Y1 = [[1],[2],[3],[4]]\n",
    "Y1 = np.array(Y1)\n",
    "\n",
    "Y_sub = Y[:,0]\n",
    "Y_sub = np.array(Y_sub)\n",
    "print Y1.shape\n",
    "print Y_sub.shape\n",
    "#for i in range(91):\n",
    "#     Y_sub = Y[:,i]\n",
    "#     #print Y_sub  \n",
    "#     X, Y_sub = make_classification(n_features=92, random_state=0)\n",
    "#     clf = LinearSVC(random_state=0)\n",
    "#     clf.fit(X, Y_sub)\n",
    "#     p = clf.predict(test_des[\"100.txt\"])\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
