{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import pylab\n",
    "import nltk\n",
    "from scipy import optimize\n",
    "from sklearn.neighbors import KDTree\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# initialize wordnet lemmatizer (need to download WordNet Corpus from NLTK)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.PorterStemmer();\n",
    "st = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = 10000\n",
    "n_test  = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resNet = []\n",
    "for line in open('./data/features_train/features_resnet1000_train.csv'):\n",
    "    resNet.append(line.strip().split(','))\n",
    "for line in resNet:\n",
    "    line[0] = int(line[0][line[0].index(\"/\")+1:line[0].index(\".\")])  \n",
    "resNet.sort()\n",
    "for i in range(len(resNet)):\n",
    "    #print resNet[i][0]\n",
    "    resNet[i] = resNet[i][1:] \n",
    "m, n = len(resNet), len(resNet[0])\n",
    "vis = np.zeros((m,n), dtype=np.float32)\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        vis[i,j] = float(resNet[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build dictionary based-on 2nd discriptions\n",
    "dictionary = []\n",
    "sentenceCounter = defaultdict(int)\n",
    "\n",
    "for i in range(n_train):  \n",
    "    for line in open('./data/descriptions_train/' + str(i) + '.txt'):\n",
    "        for w in line.strip().split('.')[0].split():\n",
    "            w = st.stem(w.lower())\n",
    "            if w not in stop_words:\n",
    "                w = wordnet_lemmatizer.lemmatize(w)\n",
    "                if len(wordnet.synsets(w)) != 0:\n",
    "                    if str(wordnet.synsets(w)[0])[str(wordnet.synsets(w)[0]).index(\".\")+1:str(wordnet.synsets(w)[0]).index(\".\")+2] == 'n':\n",
    "                        sentenceCounter[w] += 1  \n",
    "\"\"\"\"\"\"                    \n",
    "for i in range(n_test):  \n",
    "    for line in open('./data/descriptions_test/' + str(i) + '.txt'):\n",
    "        for w in line.strip().split('.')[0].split():\n",
    "            w = st.stem(w.lower())\n",
    "            if w not in stop_words:\n",
    "                w = wordnet_lemmatizer.lemmatize(w)\n",
    "                if len(wordnet.synsets(w)) != 0:\n",
    "                    if str(wordnet.synsets(w)[0])[str(wordnet.synsets(w)[0]).index(\".\")+1:str(wordnet.synsets(w)[0]).index(\".\")+2] == 'n':\n",
    "                        sentenceCounter[w] += 1   \n",
    "\"\"\"\"\"\"   \n",
    "for k,v in sentenceCounter.items():\n",
    "        #print k, v\n",
    "        dictionary.append(k)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary = []\n",
    "\n",
    "# binary BoW representation of test tags \n",
    "# tag_bow = np.zeros((n_train,len(dictionary)), dtype=np.float32)\n",
    "# for i in range(2000):\n",
    "#     for line in open('./data/tags_train/' + str(i) + '.txt'):\n",
    "#         w = line.strip().split(':')[1]\n",
    "#         if w in dictionary:\n",
    "#             tag_bow[i, dictionary.index(w)] = 1.0\n",
    "\n",
    "# binary BoW representation of train descriptions\n",
    "description_bow = np.zeros((n_train, len(dictionary)), dtype=np.float32)\n",
    "for i in range(n_train):\n",
    "    for line in open('./data/descriptions_train/' + str(i) + '.txt'):\n",
    "        for w in line.strip().split('.')[0].split():\n",
    "            w = st.stem(w.lower())\n",
    "            if w not in stop_words:\n",
    "                w = wordnet_lemmatizer.lemmatize(w)\n",
    "                if w in dictionary:\n",
    "                    description_bow[i, dictionary.index(w)] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = vis\n",
    "X = description_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary BoW representation of test descriptions\n",
    "test_bow = np.zeros((n_test,len(dictionary)), dtype=np.float32)\n",
    "for i in range(n_test):\n",
    "    for line in open('./data/descriptions_test/' + str(i) + '.txt'):\n",
    "        for w in line.strip().split('.')[0].split():\n",
    "            w = st.stem(w.lower())\n",
    "            if w not in stop_words:\n",
    "                w = wordnet_lemmatizer.lemmatize(w)\n",
    "                if w in dictionary:\n",
    "                    test_bow[i, dictionary.index(w)] = 1.0\n",
    "\n",
    "test_resNet = []\n",
    "for line in open('./data/features_test/features_resnet1000_test.csv'):\n",
    "    test_resNet.append(line.strip().split(','))\n",
    "for line in test_resNet:\n",
    "    line[0] = int(line[0][line[0].index(\"/\")+1:line[0].index(\".\")])  \n",
    "test_resNet.sort()\n",
    "for i in range(len(test_resNet)):\n",
    "    #print resNet[i][0]\n",
    "    test_resNet[i] = test_resNet[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_bow = np.vstack((description_bow, test_bow))\n",
    "total_bow.shape\n",
    "total_bow_c = total_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1750)\n",
    "total_bow = pca.fit_transform(total_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = total_bow[0:10000] # 这里是training的description\n",
    "train_y = vis # 这里是train的resNet\n",
    "# 在这里加入模型\n",
    "reg = MLPRegressor(solver='sgd', activation='logistic', shuffle=True)\n",
    "reg.fit(train_x[0:10000], train_y[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2 = MLPRegressor(solver='sgd', activation='logistic', shuffle=True)\n",
    "reg2.fit(train_y[0:10000], total_bow_c[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -7.80048998e-04  -4.18119035e-03   3.68575079e-02   1.35388812e-02\n",
      "  -3.82191851e-03   1.32645491e-02   7.45818794e-03   3.61580916e-03\n",
      "  -6.92983609e-04  -2.45474164e-03   2.01861653e-03   5.08073234e-04\n",
      "  -6.59286851e-04   5.18716992e-04   2.93124724e-03   5.09113906e-03\n",
      "  -8.28109997e-04   3.00667158e-03  -2.35562185e-03   2.65139284e-03\n",
      "  -2.89044739e-04  -4.13516753e-03   2.70556877e-03   2.91133016e-03\n",
      "   1.96478974e-03   4.49801127e-03  -3.68479945e-03   7.03438924e-03\n",
      "   3.58302552e-03  -2.65235902e-03   1.17216739e-03   2.59737328e-05\n",
      "   4.27019155e-03  -1.03495225e-03   5.04334082e-03  -2.91027763e-04\n",
      "   7.74750826e-04   5.41330428e-03  -2.48178202e-03   2.78384296e-03\n",
      "  -1.99723469e-03   6.53394791e-03   1.20223718e-03   5.21052205e-03\n",
      "  -3.59593081e-03  -5.30633531e-03  -2.88893505e-03  -3.23816418e-03\n",
      "   4.17292186e-03  -1.04144284e-03  -1.31193224e-03   3.09554642e-03\n",
      "   2.27043647e-03   2.67660176e-03  -2.71027829e-03   4.32924510e-03\n",
      "  -1.94500510e-04   3.01641721e-03  -2.40258158e-03  -3.36257185e-03\n",
      "   3.61929574e-03   3.02843094e-03   3.79698822e-04   2.00070848e-03\n",
      "   4.09786568e-03   2.85061649e-03  -3.42196121e-03   1.62203435e-03\n",
      "   9.90458245e-02   1.78649333e-03  -6.97041084e-04   2.91135366e-02\n",
      "  -3.33991809e-03  -1.46298394e-03   2.10595696e-03  -2.55844355e-03\n",
      "   3.27424080e-03  -5.36170252e-03   1.97739058e-03  -1.29734316e-03\n",
      "  -1.59698818e-03   4.32461918e-03   4.06070387e-03  -3.31643137e-03\n",
      "   1.30883649e-03   3.32870410e-03   5.08526527e-03   2.15214261e-03\n",
      "  -2.65339058e-03   1.53960998e-03  -3.15547746e-03  -5.10573900e-03\n",
      "   4.09530432e-02   1.01881049e-03   1.00430214e-02   2.25517875e-03\n",
      "  -4.81426129e-05   3.44837690e-03  -1.82662317e-03   3.78471332e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'street'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print reg2.predict(train_y)[0][0:100]\n",
    "dictionary[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2000,4125) and (1750,100) not aligned: 4125 (dim 1) != 1750 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-08ef6df38a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_resNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkNumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_bow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# output rank list on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bow_baseline.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \"\"\"\n\u001b[1;32m   1256\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coefs_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    674\u001b[0m                                          layer_units[i + 1])))\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# forward propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, activations)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[0;32m--> 104\u001b[0;31m                                                  self.coefs_[i])\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2000,4125) and (1750,100) not aligned: 4125 (dim 1) != 1750 (dim 0)"
     ]
    }
   ],
   "source": [
    "kdt = KDTree(test_resNet)\n",
    "kNumber = 20\n",
    "dist, ind = kdt.query(reg.predict(total_bow[10000:]), k=kNumber)\n",
    "# output rank list on test set\n",
    "f = open('bow_baseline.csv', 'w')\n",
    "f.write('Descritpion_ID,Top_20_Image_IDs\\n')\n",
    "for i in range(2000):\n",
    "    f.write(str(i)+'.txt,')\n",
    "    for j in range(kNumber):\n",
    "        if j == kNumber-1:\n",
    "            f.write(str(ind[i,j])+'.jpg\\n')\n",
    "        else:\n",
    "            f.write(str(ind[i,j])+'.jpg ')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdt = KDTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.25\n",
      "Score = 0.004625\n"
     ]
    }
   ],
   "source": [
    "my_data = np.loadtxt('./bow_baseline.csv', delimiter=' ', skiprows=1, dtype='str')\n",
    "counter = 0\n",
    "res = 0\n",
    "included = []\n",
    "for i in range(2000):\n",
    "    my_data[i][0] = my_data[i][0].split(',')[1]\n",
    "    correctFileName = str(counter)+'.jpg'\n",
    "    if correctFileName in my_data[i]:\n",
    "        #res += 1\n",
    "        res += (20 + 1 - np.where(my_data[i] == correctFileName)[0][0]) / 20.0\n",
    "        included.append([i, np.where(my_data[i] == correctFileName)])\n",
    "    counter += 1\n",
    "\n",
    "print(res) # 这个显示每条数据的的排序结果\n",
    "print(\"Score = \" + str(res/float(2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
