{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/Ruihao/Desktop/course/AML/HW3/data/\")\n",
    "labels = [] \n",
    "test_feature = []\n",
    "test_label = []\n",
    "train_feature = []\n",
    "train_label = []\n",
    "all_files = os.listdir(\"/Users/Ruihao/Desktop/course/AML/HW3/data\")\n",
    "txt_files = filter(lambda x: x[-4:] == '.txt', all_files)\n",
    "for filename in txt_files:\n",
    "    f = open(filename, 'r')\n",
    "    posCnt = 0; negCnt = 0\n",
    "    sampThreshold = 800\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        label = temp[1][0]\n",
    "        labels.append(label) \n",
    "        sentence = filter(None, re.split(r'[^a-zA-Z\\']', temp[0].lower()))\n",
    "        stop = set(stopwords.words('english'))\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        parsed = [lmtzr.lemmatize(word) for word in sentence if word not in stop]\n",
    "        if label == '1' and posCnt < 400:\n",
    "            train_feature.append(parsed)\n",
    "            train_label.append(label)\n",
    "            posCnt += 1\n",
    "            continue\n",
    "        if label == '0' and negCnt < 400:\n",
    "            train_feature.append(parsed)\n",
    "            train_label.append(label)\n",
    "            negCnt += 1\n",
    "            continue\n",
    "        test_feature.append(parsed)\n",
    "        test_label.append(label)\n",
    "#print train_feature[924]\n",
    "#print train_feature[1574]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BagOfWordModel(train_feature, test_feature):\n",
    "    dictionary = list()  \n",
    "    for i in range(len(train_feature)):\n",
    "        for j in range(len(train_feature[i])):\n",
    "            if train_feature[i][j] not in dictionary:\n",
    "                dictionary.append(train_feature[i][j])\n",
    "    train_feature_data = np.zeros((len(train_feature), len(dictionary)))  \n",
    "    for i in range(len(train_feature)):\n",
    "        for j in range(len(train_feature[i])):\n",
    "            train_feature_data[i][dictionary.index(train_feature[i][j])] += 1\n",
    "    test_feature_data = np.zeros((len(test_feature), len(dictionary)))      \n",
    "    for i in range(len(test_feature)):\n",
    "        for j in range(len(test_feature[i])):\n",
    "            if test_feature[i][j] in dictionary: \n",
    "                test_feature_data[i][dictionary.index(test_feature[i][j])] += 1\n",
    "    return train_feature_data, test_feature_data, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L2 Norm postprocessing strategy\n",
    "def postprocessing(train_feature_data):\n",
    "    for i in range(len(train_feature_data)):\n",
    "        x = sum([k*k for k in train_feature_data[i]])\n",
    "        x = math.sqrt(x)\n",
    "        if x == 0: continue\n",
    "        for j in range(len(dictionary)):\n",
    "            train_feature_data[i][j] /= x\n",
    "    return train_feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression based on Bag of Word Model: \n",
      "0.793333333333\n",
      "[[239  61]\n",
      " [ 63 237]]\n",
      "great\n",
      "love\n",
      "excellent\n",
      "nice\n",
      "delicious\n",
      "good\n",
      "best\n",
      "amazing\n",
      "loved\n",
      "fantastic\n",
      "***************************************************\n",
      "Naive Bayes Classifier based on Bag of Word Model: \n",
      "0.73\n",
      "[[237  63]\n",
      " [ 99 201]]\n",
      "bad\n",
      "movie\n",
      "service\n",
      "phone\n",
      "back\n",
      "time\n",
      "good\n",
      "food\n",
      "don't\n",
      "film\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "train_feature_data, test_feature_data, dictionary = BagOfWordModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "print (\"logistic regression based on Bag of Word Model: \")\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "\n",
    "print (\"***************************************************\")\n",
    "print (\"Naive Bayes Classifier based on Bag of Word Model: \")\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: print dictionary[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression based on 2-gram Model: \n",
      "0.638333333333\n",
      "[[266  34]\n",
      " [183 117]]\n",
      "work great\n",
      "highly recommend\n",
      "one best\n",
      "great phone\n",
      "great product\n",
      "food good\n",
      "easy use\n",
      "great food\n",
      "really good\n",
      "good price\n",
      "**********************************************\n",
      "Naive Bayes Classifier based on 2-gram Model: \n",
      "0.631666666667\n",
      "[[266  34]\n",
      " [187 113]]\n",
      "waste time\n",
      "waste money\n",
      "customer service\n",
      "don't waste\n",
      "won't back\n",
      "don't buy\n",
      "would recommend\n",
      "go back\n",
      "sending back\n",
      "movie bad\n"
     ]
    }
   ],
   "source": [
    "def NgramModel(train_feature, test_feature):\n",
    "    dictionary = list()\n",
    "    for i in range(len(train_feature)):\n",
    "        for j in range(len(train_feature[i])-1):\n",
    "            pair = train_feature[i][j]+\" \"+train_feature[i][j+1]\n",
    "            if pair not in dictionary:\n",
    "                dictionary.append(pair)\n",
    "    train_feature_data = np.zeros((len(train_feature), len(dictionary)))  \n",
    "    for i in range(len(train_feature)):\n",
    "        for j in range(len(train_feature[i])-1):\n",
    "            pair = train_feature[i][j] + \" \" + train_feature[i][j+1]\n",
    "            train_feature_data[i][dictionary.index(pair)] += 1\n",
    "        \n",
    "    test_feature_data = np.zeros((len(test_feature), len(dictionary)))  \n",
    "    for i in range(len(test_feature)):\n",
    "        for j in range(len(test_feature[i])-1):\n",
    "            pair = test_feature[i][j]+\" \"+test_feature[i][j+1]\n",
    "            if pair in dictionary:  \n",
    "                test_feature_data[i][dictionary.index(pair)] += 1\n",
    "    return train_feature_data, test_feature_data, dictionary\n",
    "\n",
    "train_feature_data, test_feature_data, dictionary = NgramModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "print (\"logistic regression based on 2-gram Model: \")\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "            \n",
    "print (\"**********************************************\")\n",
    "print (\"Naive Bayes Classifier based on 2-gram Model: \")\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: \n",
    "            print dictionary[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "accuracy with r = 100: logistic regression based on Bag of Word Model: \n",
      "0.738333333333\n",
      "[[230  70]\n",
      " [ 87 213]]\n",
      "razr\n",
      "phone\n",
      "mere\n",
      "problem\n",
      "design\n",
      "ft\n",
      "right\n",
      "clip\n",
      "far\n",
      "wasted\n",
      "Naive Bayes Classifier based on Bag of Word Model: \n",
      "0.631666666667\n",
      "[[211  89]\n",
      " [132 168]]\n",
      "unless\n",
      "go\n",
      "case\n",
      "u\n",
      "value\n",
      "excellent\n",
      "converter\n",
      "great\n",
      "plug\n",
      "jawbone\n",
      "logistic regression based on 2-gram Model: \n",
      "0.638333333333\n",
      "[[266  34]\n",
      " [183 117]]\n",
      "work great\n",
      "highly recommend\n",
      "one best\n",
      "great phone\n",
      "great product\n",
      "food good\n",
      "easy use\n",
      "great food\n",
      "really good\n",
      "good price\n",
      "Naive Bayes Classifier based on 2-gram Model: \n",
      "0.631666666667\n",
      "[[266  34]\n",
      " [187 113]]\n",
      "waste time\n",
      "waste money\n",
      "customer service\n",
      "don't waste\n",
      "won't back\n",
      "don't buy\n",
      "would recommend\n",
      "go back\n",
      "sending back\n",
      "movie bad\n",
      "**************************************************\n",
      "accuracy with r = 50: logistic regression based on Bag of Word Model: \n",
      "0.69\n",
      "[[233  67]\n",
      " [119 181]]\n",
      "razr\n",
      "problem\n",
      "right\n",
      "wasted\n",
      "fun\n",
      "waste\n",
      "value\n",
      "go\n",
      "jawbone\n",
      "quality\n",
      "Naive Bayes Classifier based on Bag of Word Model: \n",
      "0.628333333333\n",
      "[[204  96]\n",
      " [127 173]]\n",
      "unless\n",
      "go\n",
      "case\n",
      "u\n",
      "value\n",
      "excellent\n",
      "converter\n",
      "great\n",
      "plug\n",
      "jawbone\n",
      "logistic regression based on 2-gram Model: \n",
      "0.638333333333\n",
      "[[266  34]\n",
      " [183 117]]\n",
      "work great\n",
      "highly recommend\n",
      "one best\n",
      "great phone\n",
      "great product\n",
      "food good\n",
      "easy use\n",
      "great food\n",
      "really good\n",
      "good price\n",
      "Naive Bayes Classifier based on 2-gram Model: \n",
      "0.631666666667\n",
      "[[266  34]\n",
      " [187 113]]\n",
      "waste time\n",
      "waste money\n",
      "customer service\n",
      "don't waste\n",
      "won't back\n",
      "don't buy\n",
      "would recommend\n",
      "go back\n",
      "sending back\n",
      "movie bad\n",
      "**************************************************\n",
      "accuracy with r = 10: logistic regression based on Bag of Word Model: \n",
      "0.611666666667\n",
      "[[243  57]\n",
      " [176 124]]\n",
      "value\n",
      "go\n",
      "u\n",
      "excellent\n",
      "converter\n",
      "good\n",
      "unless\n",
      "case\n",
      "plug\n",
      "way\n",
      "Naive Bayes Classifier based on Bag of Word Model: \n",
      "0.583333333333\n",
      "[[254  46]\n",
      " [204  96]]\n",
      "unless\n",
      "go\n",
      "case\n",
      "u\n",
      "value\n",
      "excellent\n",
      "converter\n",
      "plug\n",
      "good\n",
      "way\n",
      "logistic regression based on 2-gram Model: \n",
      "0.638333333333\n",
      "[[266  34]\n",
      " [183 117]]\n",
      "work great\n",
      "highly recommend\n",
      "one best\n",
      "great phone\n",
      "great product\n",
      "food good\n",
      "easy use\n",
      "great food\n",
      "really good\n",
      "good price\n",
      "Naive Bayes Classifier based on 2-gram Model: \n",
      "0.631666666667\n",
      "[[266  34]\n",
      " [187 113]]\n",
      "waste time\n",
      "waste money\n",
      "customer service\n",
      "don't waste\n",
      "won't back\n",
      "don't buy\n",
      "would recommend\n",
      "go back\n",
      "sending back\n",
      "movie bad\n"
     ]
    }
   ],
   "source": [
    "def PCA(train, test, r):\n",
    "    # subtract mean\n",
    "    mean_vector = np.mean(train, axis=0)\n",
    "    X = train - mean_vector\n",
    "    X = np.dot(X.transpose(), X)\n",
    "    U, s, V = np.linalg.svd(X, full_matrices = True)\n",
    "    F = np.dot(train, (V[:r,:]).T)\n",
    "    K = np.dot(test, (V[:r,:]).T)\n",
    "    return F, K\n",
    "\n",
    "# test PCA implement choose r = 10, 50, 100\n",
    "print \"**************************************************\"\n",
    "print \"accuracy with r = 100:\",\n",
    "print \"logistic regression based on Bag of Word Model: \"\n",
    "train_feature_data, test_feature_data, dictionary = BagOfWordModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "train_feature_data, test_feature_data = PCA(train_feature_data, test_feature_data, 100)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "print \"Naive Bayes Classifier based on Bag of Word Model: \"\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: print dictionary[j]\n",
    "print \"logistic regression based on 2-gram Model: \"\n",
    "train_feature_data, test_feature_data, dictionary = NgramModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "print (\"Naive Bayes Classifier based on 2-gram Model: \")\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: \n",
    "            print dictionary[j]           \n",
    "print \"**************************************************\"\n",
    "print \"accuracy with r = 50:\",\n",
    "print \"logistic regression based on Bag of Word Model: \"\n",
    "train_feature_data, test_feature_data, dictionary = BagOfWordModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "train_feature_data, test_feature_data = PCA(train_feature_data, test_feature_data, 50)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "print \"Naive Bayes Classifier based on Bag of Word Model: \"\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: print dictionary[j]\n",
    "print \"logistic regression based on 2-gram Model: \"\n",
    "train_feature_data, test_feature_data, dictionary = NgramModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "print (\"Naive Bayes Classifier based on 2-gram Model: \")\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: \n",
    "            print dictionary[j]\n",
    "print \"**************************************************\"\n",
    "print \"accuracy with r = 10:\",\n",
    "print \"logistic regression based on Bag of Word Model: \"\n",
    "train_feature_data, test_feature_data, dictionary = BagOfWordModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "train_feature_data, test_feature_data = PCA(train_feature_data, test_feature_data, 10)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "print \"Naive Bayes Classifier based on Bag of Word Model: \"\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: print dictionary[j]\n",
    "print \"logistic regression based on 2-gram Model: \"\n",
    "train_feature_data, test_feature_data, dictionary = NgramModel(train_feature, test_feature)\n",
    "postprocessing(train_feature_data)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "res = logistic.fit(train_feature_data, train_label)\n",
    "print res.score(test_feature_data, test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.coef_[0])):\n",
    "        if res.coef_[0][j] == i: print dictionary[j]\n",
    "print (\"Naive Bayes Classifier based on 2-gram Model: \")\n",
    "gnb = GaussianNB()\n",
    "res = gnb.fit(train_feature_data,train_label)\n",
    "print res.score(test_feature_data,test_label)\n",
    "print confusion_matrix(test_label, res.predict(test_feature_data))\n",
    "nmax = heapq.nlargest(10, res.sigma_[0])\n",
    "for i in nmax:\n",
    "    for j in range(0, len(res.sigma_[0])):\n",
    "        if res.sigma_[0][j] == i: \n",
    "            print dictionary[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
